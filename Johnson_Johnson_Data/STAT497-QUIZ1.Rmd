---
title: "STAT/497 - QUIZ 1"
author: "BETÜL YILDIRIM"
date: "2024-12-12"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## A

```{r}
#install.packages("timeSeriesDataSets")
library(timeSeriesDataSets)
data("jj_ts")
```

```{r}
head(jj_ts)
```

```{r}
class(jj_ts)
```

```{r}
str(jj_ts)
```
```{r}
summary(jj_ts)
```
- Mean and median is close to each other.


```{r}
sum(is.na(jj_ts))
```
- There is no missing value.


```{r}
frequency(jj_ts)
```

- It is quarterly dataset.

 
 
```{r}
library(ggplot2)
library(forecast)
autoplot(jj_ts,main="Time Series Plot of jj_ts ",col="red")+theme_bw()

```


- There is an increasing trend. Mean is not constant over time. Threre is a multiplicative seasonality in this data. Volatility exists. Variance is not stable over time. The series is not stationary.


```{r}
library(gridExtra)
p1<-ggAcf(jj_ts,main="ACF of jj_ts")
p2<-ggPacf(jj_ts,main="PACF of jj_ts")
grid.arrange(p1,p2,nrow=1)
```
 


- There is a slow linear decay in ACF plot. It indicates that the series is not staionary. Also, we can see in ACF plot, there are significant spikes in lag 4, 8 and 12(there is slow decay in thsese spikes) since probably we have quarterly seasonality in this dataset. No need to interpret PACF.




## B -  Split the data into train and test set.
```{r}
traindata<-window(jj_ts ,end=c(1978,4))
testdata<-window(jj_ts ,start=c(1979,1))
length(traindata)
```

```{r}
length(jj_ts)
```

```{r}
length(testdata)
```

```{r}
lambda<-BoxCox.lambda(traindata)
lambda
```
 
```{r}
traindata_bc<-BoxCox(traindata,lambda)
```
 
```{r}
autoplot(traindata_bc)+theme_minimal()
```


- Variance is more stable by using boxcox transformation.
 
```{r}
traindata_bc_clean<-tsclean(traindata_bc)
```
 
```{r}
autoplot(traindata_bc_clean)+autolayer(traindata_bc,color="red")+theme_minimal()
```


- The function does not detect the outlier in the data.


## C and D  


- The series shows increasing trend. It is not stationary. To be sure that formal test are applied.



```{r}
library(tseries)
kpss.test(traindata_bc_clean,null="Level")
```
 
 

- The p value is less than alpha so we can reject the null hypothesis which is series is stationary. We can conclude that the series is not stationary.
 
 
 
```{r}
kpss.test(traindata_bc_clean,null="Trend")
```
 


- The p value is less than alpha so we can reject the null hypothesis which is series has deterministic trend so we can conclude that the series has stochastic trend.
 
 
 
```{r}
library(pdR)
test<-HEGY.test(traindata_bc_clean, itsd=c(1,1,0))
test$stats
```
 


- According to the HEGY test, we have both regular unit root and seasonal unit root. Therefore, we will take differences.


```{r}
ndiffs(traindata_bc_clean)
```
 
 
```{r}
nsdiffs(traindata_bc_clean)
```
 
```{r}
autoplot(diff(traindata_bc_clean))
```


- Taking regular differencing:


```{r}
test2<-HEGY.test(diff(traindata_bc_clean), itsd=c(0,0,0))
test2$stats
```
 
 
 
- There is still regular and seasonal unit root.
 
 
- Seasonal differencing:


```{r}
test3<-HEGY.test(diff(traindata_bc_clean,4), itsd=c(0,0,0))
test3$stats
```
 
 
- There is unit root.
 
 
- Taking  both regular and seasonal difference:


```{r}
test3<-HEGY.test(diff(diff(traindata_bc_clean),4), itsd=c(0,0,0))
test3$stats
```
 
- There is no seasonal root and unit root.
 
```{r}
autoplot(diff(diff(traindata_bc_clean),4))
```


- The series seems stationary.

 
```{r}
library(gridExtra)
p1<-ggAcf(diff(diff(traindata_bc_clean),4),lag.max = 48)
p2<-ggPacf(diff(diff(traindata_bc_clean),4),lag.max = 48)
grid.arrange(p1,p2,ncol=2)
```
```{r}
kpss.test(diff(diff(traindata_bc_clean),4) ,null="Level")
```


- The p value is greater than alpha so fail to reject h0. After regular and seasonal differencing, the series is now stationary.


```{r}
pp.test(diff(diff(traindata_bc_clean),4))
```


- Since the p value is less than alpha so we can reject h0 which is the series has unit root. We conclude that the series is stationary.





## E-  Identify a proper ARMA, ARIMA or SARIMA models. 

 
```{r}
library(gridExtra)
p1<-ggAcf(diff(diff(traindata_bc_clean),4),lag.max = 48)
p2<-ggPacf(diff(diff(traindata_bc_clean),4),lag.max = 48)
grid.arrange(p1,p2,ncol=2)
```


-  According to he ACF and PACF plot, we have stationary series and we can suggest models:
 
 SARIMA(0,1,1)(0,1,1)4
 SARIMA(0,1,1)(1,1,1)4
 
 SARIMA(1,1,1)(0,1,1)4
 SARIMA(1,1,1)(1,1,1)4

 
## F-  Find the best model and interpret the results.
```{r}
auto.arima(traindata_bc_clean)
```

 
```{r}
fit1<-Arima(traindata_bc_clean,order = c(0, 1, 1), seasonal = c(0, 1, 1))
fit1
```

```{r}
fit2<-Arima(traindata_bc_clean,order = c(0, 1, 1), seasonal = c(1, 1, 1))
fit2
```

```{r}
fit3<-Arima(traindata_bc_clean,order = c(1, 1, 1), seasonal = c(0, 1, 1))
fit3
```

```{r}
fit4<-Arima(traindata_bc_clean,order = c(1, 1, 1), seasonal = c(1, 1, 1))
fit4
```


```{r}
models <- list(model1 = fit1, model2 = fit2, model3 = fit3,model=fit4)
```
 
```{r}
check_significance <- function(model) {
  coefficients <- coef(model)                 
  standard_errors <- sqrt(diag(vcov(model)))  
  z_values <- abs(coefficients / standard_errors)   
  significant <- z_values > 2                 
  result <- data.frame(
    Coefficient = names(coefficients),
    Estimate = coefficients,
    Std_Error = standard_errors,
    Z_Value = z_values,
    Significant = significant
  )
  return(result)
}
```


```{r}
comparison_results <- lapply(models, function(model) {
  list(
    Significance = check_significance(model),
    AIC = AIC(model),
    BIC = BIC(model),
    Log_Likelihood = logLik(model)
  )
})
print(comparison_results)
```
```{r}
fit1
```


- fit1 has significant parameters and ıts AIC value is smallest. We should continue  fit1 model.
 
 
 
 
## G-  Apply Diagnostic Checking. Interpret the results.
 
```{r}
residuals= resid(fit1)
```
 
# Normality of Errors:
```{r}
autoplot(residuals)+geom_line(y=0)+theme_minimal()+ggtitle("Plot of The Residuals")
```


- They are scattered around zero and it can be interpreted as zero mean.
 
- QQ PLOT:
```{r}
ggplot(residuals, aes(sample = residuals)) +stat_qq()+geom_qq_line(col="red")+ggtitle("QQ Plot of the Residuals")+theme_minimal()
```


- Most of the residuals lie close to the red line, indicating that they generally follow a normal distribution.  There are slight deviations at the tails. The residuals appear mostly normally distributed, with minor deviations in the tails. 
```{r}
ggplot(residuals,aes(x=residuals))+geom_histogram(bins=16)+geom_density()+ggtitle("Histogram of Residuals")+theme_minimal()
```

```{r}
summary(residuals)
```
- The residuals are generally small, spread around zero.
 
```{r}
ggplot(residuals,aes(y=residuals,x=as.factor(1)))+geom_boxplot()+ggtitle("Box Plot of Residuals")+theme_minimal()
```


- The boxplot shows that the data seems normal. The residuals are generally well-behaved, with most of them scatered close to zero.

**Formal Tests for test the normality of residuals :**

- Jarque Bera Test
```{r}
library(tseries)
jarque.bera.test(residuals)
```
 


- According to the Jarque Bera test, the p value(0.6152) is gretar than alpha(0.05). We fail to reject h0 which is residuals have normal distribution. We conclude that residuals have normal distribution.

- Shapiro-Wilk Test
```{r}
shapiro.test(residuals)
```


- According to the Shapiro Wilk test, the p value(0.7025) is gretar than alpha(0.05).  We fail to reject h0 which is residuals have normal distribution. We conclude that residuals have normal distribution.
 
 
# Detection of the Serial Autocorrelation:
 
```{r}
ggAcf(as.vector(residuals),main="ACF of the Residuals",lag = 48)+theme_minimal()
```


- All spikes are in the WN band so the residuals are uncorrelated.

- Formal tests: 
 
- Breusch-Godfrey Test:
```{r}
library(TSA) 
m = lm(residuals ~ 1+zlag(residuals))
```
 
 
```{r}
library(lmtest)
bgtest(m,order=15) 
```
 


- Since p value(0.825) is greater than alpha, we have conclude that the residuals of the model are uncorrelated, according to results of Breusch-Godfrey Test.
 
 
- Box-Ljung Test:
```{r}
Box.test(residuals,lag=15,type = c("Ljung-Box")) 
```


- Since p value(0.8304) is greater than alpha, we have conclude that the residuals of the model are uncorrelated, according to results of Box-Ljung test.
 
 
 
# Detecting Heteroscedasticity:

```{r}
resisuals_squared=residuals^2
g1<-ggAcf(as.vector(resisuals_squared), lag.max = 48)+theme_minimal()+ggtitle("ACF of Squared Residuals")
g2<-ggPacf(as.vector(resisuals_squared), lag.max = 48)+theme_minimal()+ggtitle("PACF of Squared Residuals")  
grid.arrange(g1,g2,ncol=2)
```


- Both plots shows that almost all spikes are in of the white noise bands that is an indication of homoscedasticity. To be sure, we can  apply formal test.

- Formal test: 
- Breusch Pagan Test

```{r}
library(lmtest)
m = lm(residuals ~ traindata_bc_clean+zlag(traindata_bc_clean)+zlag(traindata_bc_clean,2))
bptest(m)
```
 


- Since p value is greater than α, we fail to reject Ho. Therefore, we can say that we have enough evidence to claim that there is no heteroscedasticity problem, according to results of Breusch-Pagan test.


- White test:

```{r}
m1 = lm(residuals ~ traindata_bc_clean+zlag(traindata_bc_clean)+zlag(traindata_bc_clean,2)+zlag(traindata_bc_clean)^2+zlag(traindata_bc_clean,2)^2+zlag(traindata_bc_clean)*zlag(traindata_bc_clean,2))
bptest(m1)
```


- Since p value is greater than alpha, we fail reject Ho. Therefore, we can say that we have enough evidence to claim that there is no heteroscedasticity problem, according to results of studentized Breusch-Pagan test.
 
- Engle’s ARCH Test: 
```{r}
#install.packages("FinTS")
library(FinTS)
ArchTest(resisuals_squared)
```
 


- The p value is greater than alpha,  we fail to reject H0 which is residuals exhibits no ARCH effects . Therefore, we can conclude that there is no presence of ARCH effects.
 
## H -  obtain forecast values from the model and calculate accuracy

```{r}
f<-forecast(fit1,h=8)
f
```

```{r}
autoplot(f)+theme_minimal()+ggtitle("Forecast of SARIMA")
```
 
```{r}
f_t<-InvBoxCox(f$mean,lambda)
```

 
```{r}
accuracy(f_t,testdata)
```
 
```{r}
autoplot(f_t,main=c("Time Series Plot of Actual Values and SARIMA Forecast"), series="forecast" ) + autolayer(testdata,series = "actual")
```


- Indicates the average absolute percentage error between the predictions and actual values. A lower MAPE is  considered a good result. Here, a value of 6.46% suggests strong accuracy in predictions.
- A smaller RMSE indicates better model accuracy. Here, 0.9551 suggests moderate error magnitude.
- Overall, we can say that the model is effective in forecasting. 